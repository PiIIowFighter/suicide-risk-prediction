{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#词频统计实验",
   "id": "614c29d8284f792e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#LLR分数选出高频词",
   "id": "78495c1560477d77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#LLR分数\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import jieba.posseg as pseg  # 用于分词和 POS 标注\n",
    "import re  # 用于处理正则表达式\n",
    "from math import log  # 用于计算对数\n",
    "\n",
    "# 自定义停用词列表（可以扩展）\n",
    "CUSTOM_STOPWORDS = {\"图片\", \"视频\", \"链接\", \"原图\", \"全文\", \"网页链接\"}\n",
    "\n",
    "# 加载数据：读取 .txt 文件并解析为 DataFrame 格式\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        # 使用制表符或空格分割文本和标签\n",
    "                        if '\\t' in line:\n",
    "                            text, label = line.rsplit('\\t', 1)\n",
    "                        else:\n",
    "                            text, label = line.rsplit(' ', 1)\n",
    "                        data.append({'text': text.strip(), 'label': int(label)})\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping invalid line: {line}\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# 清理文本：移除 \"#\" 和 \"@\" 后的内容，以及指定无意义词语和特定模式\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 移除 \"#\" 和 \"@\" 后的内容\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # 移除以 \"#\" 开头的单词（如 \"#话题\"）\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # 移除以 \"@\" 开头的单词（如 \"@用户\"）\n",
    "\n",
    "    # 移除类似 \"xxx的微博视频\" 的模式，其中 xxx 是任意字符\n",
    "    text = re.sub(r\"\\S+的微博视频\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 分组统计指定类别的频次，并获取高频单字\n",
    "def compute_pos_statistics(df, group_column, text_column):\n",
    "    pos_statistics = {}\n",
    "    \n",
    "    target_pos_tags = {\n",
    "        'Noun': ['n', 'nr', 'ns', 'nt', 'nz'],           # 名词及其子类，如人名、地名等\n",
    "        'Verb': ['v', 'vd', 'vn'],                       # 动词及其子类，如动名词等\n",
    "        'Pronouns': ['r'],                               # 代词，如“我”、“你”\n",
    "        'Adjectives': ['a', 'ad', 'an'],                 # 形容词及其子类，如副形容词等\n",
    "        'Adverbs': ['d'],                                # 副词，如“很”、“非常”\n",
    "        'Prepositions': ['p'],                           # 介词，如“在”、“从”\n",
    "        'Conjunctions': ['c']                            # 连词，如“和”、“但是”\n",
    "    }\n",
    "\n",
    "    total_texts_per_group = df[group_column].value_counts().to_dict()\n",
    "\n",
    "    for group in df[group_column].unique():\n",
    "        group_data = df[df[group_column] == group][text_column]\n",
    "\n",
    "        pos_counts = {pos: Counter() for pos in target_pos_tags}  # 初始化计数字典\n",
    "\n",
    "        for text in group_data:\n",
    "            if isinstance(text, str):  # 确保是字符串类型\n",
    "                cleaned_text = clean_text(text)  # 清理文本\n",
    "                \n",
    "                words_with_pos = pseg.cut(cleaned_text)  # 使用 jieba.posseg 对清理后的文本进行分词和 POS 标注\n",
    "                \n",
    "                for word, tag in words_with_pos:\n",
    "                    if word not in CUSTOM_STOPWORDS and word.strip():  # 排除停用词和空白字符\n",
    "                        for pos_category, pos_tags in target_pos_tags.items():\n",
    "                            if tag in pos_tags:  # 如果当前标注属于目标 POS 类别，则计数\n",
    "                                pos_counts[pos_category][word] += 1\n",
    "\n",
    "        pos_statistics[group] = {\n",
    "            'pos_counts': pos_counts,\n",
    "            'total_texts': total_texts_per_group[group],\n",
    "        }\n",
    "\n",
    "    return pos_statistics\n",
    "\n",
    "\n",
    "# 根据对数似然比 (LLR) 筛选高频动词和名词，分别输出两个组的数据，并包含出现频次\n",
    "def compute_llr(pos_statistics):\n",
    "    llr_results_group_0 = {}\n",
    "    llr_results_group_1 = {}\n",
    "\n",
    "    groups = list(pos_statistics.keys())\n",
    "    \n",
    "    if len(groups) != 2: \n",
    "        raise ValueError(\"This function expects exactly two groups to compute LLR.\")\n",
    "\n",
    "    group_0_stats, group_1_stats = pos_statistics[groups[0]], pos_statistics[groups[1]]\n",
    "\n",
    "    for pos_category in ['Noun', 'Verb']:\n",
    "        counter_0 = group_0_stats['pos_counts'][pos_category]\n",
    "        counter_1 = group_1_stats['pos_counts'][pos_category]\n",
    "\n",
    "        total_texts_0 = group_0_stats['total_texts']\n",
    "        total_texts_1 = group_1_stats['total_texts']\n",
    "\n",
    "        all_words_group_0 = set(counter_0.keys())\n",
    "        all_words_group_1 = set(counter_1.keys())\n",
    "\n",
    "        llr_scores_group_0 = []\n",
    "        llr_scores_group_1 = []\n",
    "\n",
    "        for word in all_words_group_0:\n",
    "            N11 = counter_0[word]\n",
    "            N01 = counter_1[word]\n",
    "            N10 = total_texts_0 - N11\n",
    "            N00 = total_texts_1 - N01\n",
    "\n",
    "            if N11 > 0 and N01 > 0 and N10 > 0 and N00 > 0:  \n",
    "                llr_score_group_0 = log((N11 * N00) / (N10 * N01))\n",
    "                llr_scores_group_0.append((word, llr_score_group_0, N11))\n",
    "\n",
    "        for word in all_words_group_1:\n",
    "            N11 = counter_1[word]\n",
    "            N01 = counter_0[word]\n",
    "            N10 = total_texts_1 - N11\n",
    "            N00 = total_texts_0 - N01\n",
    "\n",
    "            if N11 > 0 and N01 > 0 and N10 > 0 and N00 > 0:  \n",
    "                llr_score_group_1 = log((N11 * N00) / (N10 * N01))\n",
    "                llr_scores_group_1.append((word, llr_score_group_1, N11))\n",
    "\n",
    "        llr_results_group_0[pos_category] = sorted(llr_scores_group_0, key=lambda x: x[1], reverse=True)[:100]\n",
    "        llr_results_group_1[pos_category] = sorted(llr_scores_group_1, key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "    return llr_results_group_0, llr_results_group_1\n",
    "\n",
    "\n",
    "# 主函数：加载数据并计算结果，并生成表格输出\n",
    "def main():\n",
    "    file_paths = ['/kaggle/input/cipintongji/dev.txt','/kaggle/input/cipintongji/train.txt']  # 替换为实际文件路径\n",
    "\n",
    "    df = load_data(file_paths)\n",
    "\n",
    "    print(\"\\nLoaded Data:\")\n",
    "    print(df.head())\n",
    "\n",
    "    pos_statistics = compute_pos_statistics(df, group_column='label', text_column='text')\n",
    "\n",
    "    print(\"\\nComputing LLR Scores...\")\n",
    "    \n",
    "    llr_results_group_0, llr_results_group_1 = compute_llr(pos_statistics)\n",
    "\n",
    "    print(\"\\nTop Words by LLR (Group: Suicide Risk):\")\n",
    "    \n",
    "    for pos_category, words_info in llr_results_group_0.items():\n",
    "        print(f\"\\nTop {pos_category}:\")\n",
    "        for word, score, freq in words_info:\n",
    "            print(f\"Word: {word}, LLR Score: {score:.4f}, Frequency: {freq}\")\n",
    "\n",
    "    print(\"\\nTop Words by LLR (Group: No Suicide Risk):\")\n",
    "    \n",
    "    for pos_category, words_info in llr_results_group_1.items():\n",
    "        print(f\"\\nTop {pos_category}:\")\n",
    "        for word, score, freq in words_info:\n",
    "            print(f\"Word: {word}, LLR Score: {score:.4f}, Frequency: {freq}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "40430cfb788d4971"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#词频统计",
   "id": "705fdda06094316b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T11:41:44.038257Z",
     "start_time": "2025-01-14T11:41:43.027694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import jieba.posseg as pseg  # 用于分词和 POS 标注\n",
    "import re  # 用于处理正则表达式\n",
    "from prettytable import PrettyTable  # 用于绘制表格\n",
    "\n",
    "# 自定义停用词列表（可以扩展）\n",
    "CUSTOM_STOPWORDS = {\"图片\", \"视频\", \"链接\", \"原图\", \"全文\", \"网页链接\"}\n",
    "\n",
    "# 加载数据：读取 .txt 文件并解析为 DataFrame 格式\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        # 使用制表符或空格分割文本和标签\n",
    "                        if '\\t' in line:\n",
    "                            text, label = line.rsplit('\\t', 1)\n",
    "                        else:\n",
    "                            text, label = line.rsplit(' ', 1)\n",
    "                        data.append({'text': text.strip(), 'label': int(label)})\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping invalid line: {line}\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# 清理文本：移除 \"#\" 和 \"@\" 后的内容，以及指定无意义词语和特定模式\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 移除 \"#\" 和 \"@\" 后的内容\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # 移除以 \"#\" 开头的单词（如 \"#话题\"）\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # 移除以 \"@\" 开头的单词（如 \"@用户\"）\n",
    "\n",
    "    # 移除类似 \"xxx的微博视频\" 的模式，其中 xxx 是任意字符\n",
    "    text = re.sub(r\"\\S+的微博视频\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 分组统计指定类别的频次，并获取高频单字\n",
    "def compute_pos_statistics(df, group_column, text_column):\n",
    "    pos_statistics = {}\n",
    "    \n",
    "    # 定义目标 POS 标签映射表\n",
    "    target_pos_tags = {\n",
    "        'Noun': ['n', 'nr', 'ns', 'nt', 'nz'],           # 名词及其子类，如人名、地名等\n",
    "        'Verb': ['v', 'vd', 'vn'],                       # 动词及其子类，如动名词等\n",
    "        'Pronouns': ['r'],                               # 代词，如“我”、“你”\n",
    "        'Adjectives': ['a', 'ad', 'an'],                 # 形容词及其子类，如副形容词等\n",
    "        'Adverbs': ['d'],                                # 副词，如“很”、“非常”\n",
    "        'Prepositions': ['p'],                           # 介词，如“在”、“从”\n",
    "        'Conjunctions': ['c']                            # 连词，如“和”、“但是”\n",
    "    }\n",
    "\n",
    "    for group in df[group_column].unique():\n",
    "        group_data = df[df[group_column] == group][text_column]\n",
    "\n",
    "        pos_counts = {pos: Counter() for pos in target_pos_tags}  # 初始化计数字典\n",
    "\n",
    "        for text in group_data:\n",
    "            if isinstance(text, str):  # 确保是字符串类型\n",
    "                cleaned_text = clean_text(text)  # 清理文本\n",
    "                \n",
    "                words_with_pos = pseg.cut(cleaned_text)  # 使用 jieba.posseg 对清理后的文本进行分词和 POS 标注\n",
    "                \n",
    "                for word, tag in words_with_pos:\n",
    "                    if word not in CUSTOM_STOPWORDS and word.strip():  # 排除停用词和空白字符\n",
    "                        for pos_category, pos_tags in target_pos_tags.items():\n",
    "                            if tag in pos_tags:  # 如果当前标注属于目标 POS 类别，则计数\n",
    "                                pos_counts[pos_category][word] += 1\n",
    "\n",
    "        total_count = sum(sum(counter.values()) for counter in pos_counts.values())\n",
    "\n",
    "        pos_statistics[group] = {\n",
    "            'pos_counts': pos_counts,\n",
    "            'total_count': total_count,\n",
    "            'percentage': {\n",
    "                pos_category: {word: (count / total_count * 100) for word, count in counter.items()}\n",
    "                for pos_category, counter in pos_counts.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return pos_statistics\n",
    "\n",
    "\n",
    "# 绘制表格：按组展示各类 POS 的总频次及占比\n",
    "def generate_table(pos_statistics):\n",
    "    table = PrettyTable()\n",
    "    \n",
    "    table.field_names = [\"POS Category\", \"Group\", \"Frequency\", \"Percentage (%)\"]\n",
    "    \n",
    "    for group, stats in pos_statistics.items():\n",
    "        total_count = stats['total_count']\n",
    "        \n",
    "        for pos_category, counter in stats['pos_counts'].items():\n",
    "            category_total = sum(counter.values())\n",
    "            table.add_row([pos_category, \n",
    "                           \"Suicide Risk\" if group == 0 else \"No Suicide Risk\",\n",
    "                           category_total,\n",
    "                           f\"{category_total / total_count * 100:.2f}\"])\n",
    "    \n",
    "    return table\n",
    "\n",
    "\n",
    "# 获取前 N 个高频单字及其占比（针对每个 POS 类别）\n",
    "def get_top_n_words(pos_statistics, top_n=200):\n",
    "    top_words_by_group = {}\n",
    "    \n",
    "    for group, stats in pos_statistics.items():\n",
    "        top_words_by_group[group] = {}\n",
    "        \n",
    "        for pos_category, counter in stats['pos_counts'].items():\n",
    "            top_words_by_group[group][pos_category] = [\n",
    "                (word, count, stats['percentage'][pos_category][word])\n",
    "                for word, count in counter.most_common(top_n)\n",
    "            ]\n",
    "    \n",
    "    return top_words_by_group\n",
    "\n",
    "\n",
    "# 主函数：加载数据并计算结果，并生成表格输出\n",
    "def main():\n",
    "    # 文件路径\n",
    "    file_paths = ['/kaggle/input/cipintongji/dev.txt', '/kaggle/input/cipintongji/train.txt']  # 替换为实际文件路径\n",
    "\n",
    "    # 加载数据\n",
    "    df = load_data(file_paths)\n",
    "\n",
    "    print(\"\\nLoaded Data:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # 计算词性统计信息\n",
    "    pos_statistics = compute_pos_statistics(df, group_column='label', text_column='text')\n",
    "\n",
    "    print(\"\\nComputing LLR Scores...\")\n",
    "\n",
    "    # 计算 LLR 分数\n",
    "    llr_results_group_0, llr_results_group_1 = compute_llr(pos_statistics)\n",
    "\n",
    "    print(\"\\nTop Words by LLR (Group: Suicide Risk):\")\n",
    "    \n",
    "    for pos_category, words_info in llr_results_group_0.items():\n",
    "        print(f\"\\nTop {pos_category}:\")\n",
    "        for word, score, freq, freq_ratio in words_info:\n",
    "            print(f\"Word: {word}, LLR Score: {score:.4f}, Frequency: {freq}, Frequency Ratio: {freq_ratio:.4%}\")\n",
    "\n",
    "    print(\"\\nTop Words by LLR (Group: No Suicide Risk):\")\n",
    "    \n",
    "    for pos_category, words_info in llr_results_group_1.items():\n",
    "        print(f\"\\nTop {pos_category}:\")\n",
    "        for word, score, freq, freq_ratio in words_info:\n",
    "            print(f\"Word: {word}, LLR Score: {score:.4f}, Frequency: {freq}, Frequency Ratio: {freq_ratio:.4%}\")\n",
    "  if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "cf9e4d3affa572ef",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "人称统计",
   "id": "9aadcc5d345335d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#统计第一人称 第二人称 第三人称\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import jieba.posseg as pseg  # 用于分词和 POS 标注\n",
    "import re  # 用于处理正则表达式\n",
    "from prettytable import PrettyTable  # 用于绘制表格\n",
    "\n",
    "# 自定义停用词列表（可以扩展）\n",
    "CUSTOM_STOPWORDS = {\"图片\", \"视频\", \"链接\", \"原图\", \"全文\", \"网页链接\"}\n",
    "\n",
    "# 定义代词类别及对应的词汇表\n",
    "PRONOUN_CATEGORIES = {\n",
    "    'First Person Singular': {\"我\", \"自己\",\"俺\",\"咱\"},\n",
    "    'First Person Plural': {\"我们\", \"咱们\"},\n",
    "    'Second Person': {\"你\", \"你们\",\"您\",\"您们\"},\n",
    "    'Third Person': {\"他\", \"她\", \"他们\", \"她们\", \"爸爸\" ,\"妈妈\",\"哥哥\",\"姐姐\",\"叔叔\",\"大伯\"}\n",
    "}\n",
    "\n",
    "# 加载数据：读取 .txt 文件并解析为 DataFrame 格式\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        # 使用制表符或空格分割文本和标签\n",
    "                        if '\\t' in line:\n",
    "                            text, label = line.rsplit('\\t', 1)\n",
    "                        else:\n",
    "                            text, label = line.rsplit(' ', 1)\n",
    "                        data.append({'text': text.strip(), 'label': int(label)})\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping invalid line: {line}\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# 清理文本：移除 \"#\" 和 \"@\" 后的内容，以及指定无意义词语和特定模式\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 移除 \"#\" 和 \"@\" 后的内容\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # 移除以 \"#\" 开头的单词（如 \"#话题\"）\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # 移除以 \"@\" 开头的单词（如 \"@用户\"）\n",
    "\n",
    "    # 移除类似 \"xxx的微博视频\" 的模式，其中 xxx 是任意字符\n",
    "    text = re.sub(r\"\\S+的微博视频\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 分组统计代词类别的频次，并获取高频单字\n",
    "def compute_pronoun_statistics(df, group_column, text_column):\n",
    "    pronoun_statistics = {}\n",
    "\n",
    "    for group in df[group_column].unique():\n",
    "        group_data = df[df[group_column] == group][text_column]\n",
    "\n",
    "        pronoun_counts = {category: Counter() for category in PRONOUN_CATEGORIES}  # 初始化计数字典\n",
    "\n",
    "        for text in group_data:\n",
    "            if isinstance(text, str):  # 确保是字符串类型\n",
    "                cleaned_text = clean_text(text)  # 清理文本\n",
    "                \n",
    "                words_with_pos = pseg.cut(cleaned_text)  # 使用 jieba.posseg 对清理后的文本进行分词和 POS 标注\n",
    "                \n",
    "                for word, tag in words_with_pos:\n",
    "                    if word not in CUSTOM_STOPWORDS and word.strip() and tag == 'r':  # 排除非代词和空白字符\n",
    "                        for category, pronouns in PRONOUN_CATEGORIES.items():\n",
    "                            if word in pronouns:  # 如果当前单词属于某个代词类别，则计数\n",
    "                                pronoun_counts[category][word] += 1\n",
    "\n",
    "        total_count = sum(sum(counter.values()) for counter in pronoun_counts.values())\n",
    "\n",
    "        pronoun_statistics[group] = {\n",
    "            'pronoun_counts': pronoun_counts,\n",
    "            'total_count': total_count,\n",
    "            'percentage': {\n",
    "                category: {word: (count / total_count * 100) for word, count in counter.items()}\n",
    "                for category, counter in pronoun_counts.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return pronoun_statistics\n",
    "\n",
    "\n",
    "# 绘制表格：按组展示各类代词的总频次及占比\n",
    "def generate_table(pronoun_statistics):\n",
    "    table = PrettyTable()\n",
    "    \n",
    "    table.field_names = [\"Pronoun Category\", \"Group\", \"Frequency\", \"Percentage (%)\"]\n",
    "    \n",
    "    for group, stats in pronoun_statistics.items():\n",
    "        total_count = stats['total_count']\n",
    "        \n",
    "        for category, counter in stats['pronoun_counts'].items():\n",
    "            category_total = sum(counter.values())\n",
    "            table.add_row([category, \n",
    "                           \"Suicide Risk\" if group == 0 else \"No Suicide Risk\",\n",
    "                           category_total,\n",
    "                           f\"{category_total / total_count * 100:.2f}\"])\n",
    "    \n",
    "    return table\n",
    "\n",
    "\n",
    "# 获取前 N 个高频单字及其占比（针对每个代词类别）\n",
    "def get_top_n_pronouns(pronoun_statistics, top_n=20):\n",
    "    top_words_by_group = {}\n",
    "    \n",
    "    for group, stats in pronoun_statistics.items():\n",
    "        top_words_by_group[group] = {}\n",
    "        \n",
    "        for category, counter in stats['pronoun_counts'].items():\n",
    "            top_words_by_group[group][category] = [\n",
    "                (word, count, stats['percentage'][category][word])\n",
    "                for word, count in counter.most_common(top_n)\n",
    "            ]\n",
    "    \n",
    "    return top_words_by_group\n",
    "\n",
    "\n",
    "# 主函数：加载数据并计算结果，并生成表格输出\n",
    "def main():\n",
    "    file_paths = ['/kaggle/input/cipintongji/dev.txt','/kaggle/input/cipintongji/train.txt']  # 替换为实际文件路径\n",
    "\n",
    "    # 加载数据（假设文件格式为“一段中文 后面标有0或1”）\n",
    "    df = load_data(file_paths)\n",
    "\n",
    "    print(\"\\nLoaded Data:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # 分析每组中各类代词的统计信息，仅针对目标类别进行分析，并清理无意义内容\n",
    "    pronoun_statistics = compute_pronoun_statistics(df, group_column='label', text_column='text')\n",
    "\n",
    "    print(\"\\nPronoun Statistics Table:\")\n",
    "    \n",
    "    result_table = generate_table(pronoun_statistics)\n",
    "    \n",
    "    print(result_table)\n",
    "\n",
    "    print(\"\\nTop Pronouns by Group:\")\n",
    "    \n",
    "    top_pronouns_by_group = get_top_n_pronouns(pronoun_statistics)\n",
    "\n",
    "    for group, words_info in top_pronouns_by_group.items():\n",
    "        print(f\"\\nGroup {'Suicide Risk' if group == 0 else 'No Suicide Risk'}:\")\n",
    "        \n",
    "        for category, words_list in words_info.items():\n",
    "            print(f\"\\nTop {category}:\")\n",
    "            for word_info in words_list:\n",
    "                print(f\"Word: {word_info[0]}, Count: {word_info[1]}, Percentage: {word_info[2]:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "7763ea2e4a91a51e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T08:00:40.488432Z",
     "start_time": "2025-01-14T08:00:38.239618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据\n",
    "nouns_control = ['depression', 'friend', 'thing', 'time', 'get', 'boyfriend', 'relationship', 'day', 'people', 'anxiety']\n",
    "nouns_depression = ['depression', 'friend', 'thing', 'time', 'get', 'boyfriend', 'relationship', 'day', 'people', 'anxiety']\n",
    "verbs_control = ['be', 'feel', 'have', 'want', 'go', 'do', 'talk', 'know', 'thank', 'love']\n",
    "verbs_depression = ['be', 'feel', 'have', 'want', 'go', 'do', 'talk', 'know', 'thank', 'love']\n",
    "\n",
    "# 假设的频率数据\n",
    "noun_freq_control = [4000, 3500, 3000, 2500, 2000, 1800, 1600, 1400, 1200, 1000]\n",
    "noun_freq_depression = [3800, 3300, 2800, 2300, 1800, 1600, 1400, 1200, 1000, 900]\n",
    "verb_freq_control = [3000, 2800, 2600, 2400, 2200, 2000, 1800, 1600, 1400, 1200]\n",
    "verb_freq_depression = [2900, 2700, 2500, 2300, 2100, 1900, 1700, 1500, 1300, 1100]\n",
    "\n",
    "# 创建图形和轴\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# 绘制名词条形图\n",
    "ax1.barh(nouns_control, noun_freq_control, color='green', label='Control')\n",
    "ax1.barh(nouns_depression, noun_freq_depression, color='blue', label='Depression')\n",
    "ax1.set_title('Keyness Nouns')\n",
    "ax1.set_xlabel('Frequency')\n",
    "ax1.legend()\n",
    "\n",
    "# 绘制动词条形图\n",
    "ax2.barh(verbs_control, verb_freq_control, color='green', label='Control')\n",
    "ax2.barh(verbs_depression, verb_freq_depression, color='blue', label='Depression')\n",
    "ax2.set_title('Keyness Verbs')\n",
    "ax2.set_xlabel('Frequency')\n",
    "ax2.legend()\n",
    "\n",
    "# 显示图形\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d90e724ba9c2ac2c",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PolyCollection:kwdoc'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[1;32mC:\\Data_Anaconda\\Lib\\site-packages\\matplotlib\\_docstring.py:57\u001B[0m, in \u001B[0;36m_ArtistKwdocLoader.__missing__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 57\u001B[0m     \u001B[38;5;28mcls\u001B[39m, \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m _api\u001B[38;5;241m.\u001B[39mrecursive_subclasses(Artist)\n\u001B[0;32m     58\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m name]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 1)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# 数据\u001B[39;00m\n\u001B[0;32m      4\u001B[0m nouns_control \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdepression\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfriend\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthing\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mget\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboyfriend\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelationship\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mday\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpeople\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124manxiety\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32mC:\\Data_Anaconda\\Lib\\site-packages\\matplotlib\\pyplot.py:66\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _docstring\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend_bases\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     65\u001B[0m     FigureCanvasBase, FigureManagerBase, MouseButton)\n\u001B[1;32m---> 66\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfigure\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Figure, FigureBase, figaspect\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgridspec\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GridSpec, SubplotSpec\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rcsetup, rcParamsDefault, rcParamsOrig\n",
      "File \u001B[1;32mC:\\Data_Anaconda\\Lib\\site-packages\\matplotlib\\figure.py:43\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmpl\u001B[39;00m\n\u001B[1;32m---> 43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _blocking_input, backend_bases, _docstring, projections\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01martist\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     45\u001B[0m     Artist, allow_rasterization, _finalize_rasterization)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend_bases\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     47\u001B[0m     DrawEvent, FigureCanvasBase, NonGuiException, MouseButton, _get_renderer)\n",
      "File \u001B[1;32mC:\\Data_Anaconda\\Lib\\site-packages\\matplotlib\\projections\\__init__.py:55\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mNon-separable transforms that map from data space to screen space.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;124;03m`matplotlib.projections.polar` may also be of interest.\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m axes, _docstring\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeo\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AitoffAxes, HammerAxes, LambertAxes, MollweideAxes\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpolar\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PolarAxes\n",
      "File \u001B[1;32mC:\\Data_Anaconda\\Lib\\site-packages\\matplotlib\\axes\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _base\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_axes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Backcompat.\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_axes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Axes \u001B[38;5;28;01mas\u001B[39;00m Subplot\n",
      "File \u001B[1;32mC:\\Data_Anaconda\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:24\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpatches\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmpatches\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpath\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmpath\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mquiver\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmquiver\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstackplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmstack\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstreamplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmstream\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Data_Anaconda\\Lib\\site-packages\\matplotlib\\quiver.py:30\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmtext\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtransforms\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m _quiver_doc \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;124mPlot a 2D field of arrows.\u001B[39m\n\u001B[0;32m     32\u001B[0m \n\u001B[0;32m     33\u001B[0m \u001B[38;5;124mCall signature::\u001B[39m\n\u001B[0;32m     34\u001B[0m \n\u001B[0;32m     35\u001B[0m \u001B[38;5;124m  quiver([X, Y], U, V, [C], **kwargs)\u001B[39m\n\u001B[0;32m     36\u001B[0m \n\u001B[0;32m     37\u001B[0m \u001B[38;5;124m*X*, *Y* define the arrow locations, *U*, *V* define the arrow directions, and\u001B[39m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;124m*C* optionally sets the color.\u001B[39m\n\u001B[0;32m     39\u001B[0m \n\u001B[0;32m     40\u001B[0m \u001B[38;5;124m**Arrow length**\u001B[39m\n\u001B[0;32m     41\u001B[0m \n\u001B[0;32m     42\u001B[0m \u001B[38;5;124mThe default settings auto-scales the length of the arrows to a reasonable size.\u001B[39m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;124mTo change this behavior see the *scale* and *scale_units* parameters.\u001B[39m\n\u001B[0;32m     44\u001B[0m \n\u001B[0;32m     45\u001B[0m \u001B[38;5;124m**Arrow shape**\u001B[39m\n\u001B[0;32m     46\u001B[0m \n\u001B[0;32m     47\u001B[0m \u001B[38;5;124mThe arrow shape is determined by *width*, *headwidth*, *headlength* and\u001B[39m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;124m*headaxislength*. See the notes below.\u001B[39m\n\u001B[0;32m     49\u001B[0m \n\u001B[0;32m     50\u001B[0m \u001B[38;5;124m**Arrow styling**\u001B[39m\n\u001B[0;32m     51\u001B[0m \n\u001B[0;32m     52\u001B[0m \u001B[38;5;124mEach arrow is internally represented by a filled polygon with a default edge\u001B[39m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;124mlinewidth of 0. As a result, an arrow is rather a filled area, not a line with\u001B[39m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;124ma head, and `.PolyCollection` properties like *linewidth*, *edgecolor*,\u001B[39m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;124m*facecolor*, etc. act accordingly.\u001B[39m\n\u001B[0;32m     56\u001B[0m \n\u001B[0;32m     57\u001B[0m \n\u001B[0;32m     58\u001B[0m \u001B[38;5;124mParameters\u001B[39m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;124m----------\u001B[39m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;124mX, Y : 1D or 2D array-like, optional\u001B[39m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;124m    The x and y coordinates of the arrow locations.\u001B[39m\n\u001B[0;32m     62\u001B[0m \n\u001B[0;32m     63\u001B[0m \u001B[38;5;124m    If not given, they will be generated as a uniform integer meshgrid based\u001B[39m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;124m    on the dimensions of *U* and *V*.\u001B[39m\n\u001B[0;32m     65\u001B[0m \n\u001B[0;32m     66\u001B[0m \u001B[38;5;124m    If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\u001B[39m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;124m    using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\u001B[39m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;124m    must match the column and row dimensions of *U* and *V*.\u001B[39m\n\u001B[0;32m     69\u001B[0m \n\u001B[0;32m     70\u001B[0m \u001B[38;5;124mU, V : 1D or 2D array-like\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124m    The x and y direction components of the arrow vectors. The interpretation\u001B[39m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;124m    of these components (in data or in screen space) depends on *angles*.\u001B[39m\n\u001B[0;32m     73\u001B[0m \n\u001B[0;32m     74\u001B[0m \u001B[38;5;124m    *U* and *V* must have the same number of elements, matching the number of\u001B[39m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;124m    arrow locations in  *X*, *Y*. *U* and *V* may be masked. Locations masked\u001B[39m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;124m    in any of *U*, *V*, and *C* will not be drawn.\u001B[39m\n\u001B[0;32m     77\u001B[0m \n\u001B[0;32m     78\u001B[0m \u001B[38;5;124mC : 1D or 2D array-like, optional\u001B[39m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;124m    Numeric data that defines the arrow colors by colormapping via *norm* and\u001B[39m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;124m    *cmap*.\u001B[39m\n\u001B[0;32m     81\u001B[0m \n\u001B[0;32m     82\u001B[0m \u001B[38;5;124m    This does not support explicit colors. If you want to set colors directly,\u001B[39m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;124m    use *color* instead.  The size of *C* must match the number of arrow\u001B[39m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;124m    locations.\u001B[39m\n\u001B[0;32m     85\u001B[0m \n\u001B[0;32m     86\u001B[0m \u001B[38;5;124mangles : \u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muv\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxy\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m} or array-like, default: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muv\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;124m    Method for determining the angle of the arrows.\u001B[39m\n\u001B[0;32m     88\u001B[0m \n\u001B[0;32m     89\u001B[0m \u001B[38;5;124m    - \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muv\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: Arrow direction in screen coordinates. Use this if the arrows\u001B[39m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;124m      symbolize a quantity that is not based on *X*, *Y* data coordinates.\u001B[39m\n\u001B[0;32m     91\u001B[0m \n\u001B[0;32m     92\u001B[0m \u001B[38;5;124m      If *U* == *V* the orientation of the arrow on the plot is 45 degrees\u001B[39m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;124m      counter-clockwise from the  horizontal axis (positive to the right).\u001B[39m\n\u001B[0;32m     94\u001B[0m \n\u001B[0;32m     95\u001B[0m \u001B[38;5;124m    - \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxy\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: Arrow direction in data coordinates, i.e. the arrows point from\u001B[39m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;124m      (x, y) to (x+u, y+v). Use this e.g. for plotting a gradient field.\u001B[39m\n\u001B[0;32m     97\u001B[0m \n\u001B[0;32m     98\u001B[0m \u001B[38;5;124m    - Arbitrary angles may be specified explicitly as an array of values\u001B[39m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;124m      in degrees, counter-clockwise from the horizontal axis.\u001B[39m\n\u001B[0;32m    100\u001B[0m \n\u001B[0;32m    101\u001B[0m \u001B[38;5;124m      In this case *U*, *V* is only used to determine the length of the\u001B[39m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;124m      arrows.\u001B[39m\n\u001B[0;32m    103\u001B[0m \n\u001B[0;32m    104\u001B[0m \u001B[38;5;124m    Note: inverting a data axis will correspondingly invert the\u001B[39m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;124m    arrows only with ``angles=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxy\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m``.\u001B[39m\n\u001B[0;32m    106\u001B[0m \n\u001B[0;32m    107\u001B[0m \u001B[38;5;124mpivot : \u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtail\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmid\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmiddle\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtip\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m}, default: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtail\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;124m    The part of the arrow that is anchored to the *X*, *Y* grid. The arrow\u001B[39m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;124m    rotates about this point.\u001B[39m\n\u001B[0;32m    110\u001B[0m \n\u001B[0;32m    111\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmid\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is a synonym for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmiddle\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\n\u001B[0;32m    112\u001B[0m \n\u001B[0;32m    113\u001B[0m \u001B[38;5;124mscale : float, optional\u001B[39m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;124m    Scales the length of the arrow inversely.\u001B[39m\n\u001B[0;32m    115\u001B[0m \n\u001B[0;32m    116\u001B[0m \u001B[38;5;124m    Number of data units per arrow length unit, e.g., m/s per plot width; a\u001B[39m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;124m    smaller scale parameter makes the arrow longer. Default is *None*.\u001B[39m\n\u001B[0;32m    118\u001B[0m \n\u001B[0;32m    119\u001B[0m \u001B[38;5;124m    If *None*, a simple autoscaling algorithm is used, based on the average\u001B[39m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;124m    vector length and the number of vectors. The arrow length unit is given by\u001B[39m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;124m    the *scale_units* parameter.\u001B[39m\n\u001B[0;32m    122\u001B[0m \n\u001B[0;32m    123\u001B[0m \u001B[38;5;124mscale_units : \u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdots\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minches\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxy\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m}, optional\u001B[39m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;124m    If the *scale* kwarg is *None*, the arrow length unit. Default is *None*.\u001B[39m\n\u001B[0;32m    125\u001B[0m \n\u001B[0;32m    126\u001B[0m \u001B[38;5;124m    e.g. *scale_units* is \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minches\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, *scale* is 2.0, and ``(u, v) = (1, 0)``,\u001B[39m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;124m    then the vector will be 0.5 inches long.\u001B[39m\n\u001B[0;32m    128\u001B[0m \n\u001B[0;32m    129\u001B[0m \u001B[38;5;124m    If *scale_units* is \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, then the vector will be half the\u001B[39m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;124m    width/height of the axes.\u001B[39m\n\u001B[0;32m    131\u001B[0m \n\u001B[0;32m    132\u001B[0m \u001B[38;5;124m    If *scale_units* is \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m then the vector will be 0.5 x-axis\u001B[39m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;124m    units. To plot vectors in the x-y plane, with u and v having\u001B[39m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;124m    the same units as x and y, use\u001B[39m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124m    ``angles=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxy\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, scale_units=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxy\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, scale=1``.\u001B[39m\n\u001B[0;32m    136\u001B[0m \n\u001B[0;32m    137\u001B[0m \u001B[38;5;124munits : \u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdots\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minches\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxy\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m}, default: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;124m    Affects the arrow size (except for the length). In particular, the shaft\u001B[39m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;124m    *width* is measured in multiples of this unit.\u001B[39m\n\u001B[0;32m    140\u001B[0m \n\u001B[0;32m    141\u001B[0m \u001B[38;5;124m    Supported values are:\u001B[39m\n\u001B[0;32m    142\u001B[0m \n\u001B[0;32m    143\u001B[0m \u001B[38;5;124m    - \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: The width or height of the Axes.\u001B[39m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;124m    - \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdots\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minches\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: Pixels or inches based on the figure dpi.\u001B[39m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;124m    - \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxy\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: *X*, *Y* or :math:`\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124msqrt\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124mX^2 + Y^2}` in data units.\u001B[39m\n\u001B[0;32m    146\u001B[0m \n\u001B[0;32m    147\u001B[0m \u001B[38;5;124m    The following table summarizes how these values affect the visible arrow\u001B[39m\n\u001B[0;32m    148\u001B[0m \u001B[38;5;124m    size under zooming and figure size changes:\u001B[39m\n\u001B[0;32m    149\u001B[0m \n\u001B[0;32m    150\u001B[0m \u001B[38;5;124m    =================  =================   ==================\u001B[39m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;124m    units              zoom                figure size change\u001B[39m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;124m    =================  =================   ==================\u001B[39m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxy\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m     arrow size scales   —\u001B[39m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m  —                   arrow size scales\u001B[39m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdots\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minches\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m   —                   —\u001B[39m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;124m    =================  =================   ==================\u001B[39m\n\u001B[0;32m    157\u001B[0m \n\u001B[0;32m    158\u001B[0m \u001B[38;5;124mwidth : float, optional\u001B[39m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;124m    Shaft width in arrow units. All head parameters are relative to *width*.\u001B[39m\n\u001B[0;32m    160\u001B[0m \n\u001B[0;32m    161\u001B[0m \u001B[38;5;124m    The default depends on choice of *units* above, and number of vectors;\u001B[39m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;124m    a typical starting value is about 0.005 times the width of the plot.\u001B[39m\n\u001B[0;32m    163\u001B[0m \n\u001B[0;32m    164\u001B[0m \u001B[38;5;124mheadwidth : float, default: 3\u001B[39m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;124m    Head width as multiple of shaft *width*. See the notes below.\u001B[39m\n\u001B[0;32m    166\u001B[0m \n\u001B[0;32m    167\u001B[0m \u001B[38;5;124mheadlength : float, default: 5\u001B[39m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;124m    Head length as multiple of shaft *width*. See the notes below.\u001B[39m\n\u001B[0;32m    169\u001B[0m \n\u001B[0;32m    170\u001B[0m \u001B[38;5;124mheadaxislength : float, default: 4.5\u001B[39m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;124m    Head length at shaft intersection as multiple of shaft *width*.\u001B[39m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;124m    See the notes below.\u001B[39m\n\u001B[0;32m    173\u001B[0m \n\u001B[0;32m    174\u001B[0m \u001B[38;5;124mminshaft : float, default: 1\u001B[39m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;124m    Length below which arrow scales, in units of head length. Do not\u001B[39m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;124m    set this to less than 1, or small arrows will look terrible!\u001B[39m\n\u001B[0;32m    177\u001B[0m \n\u001B[0;32m    178\u001B[0m \u001B[38;5;124mminlength : float, default: 1\u001B[39m\n\u001B[0;32m    179\u001B[0m \u001B[38;5;124m    Minimum length as a multiple of shaft width; if an arrow length\u001B[39m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;124m    is less than this, plot a dot (hexagon) of this diameter instead.\u001B[39m\n\u001B[0;32m    181\u001B[0m \n\u001B[0;32m    182\u001B[0m \u001B[38;5;124mcolor : color or color sequence, optional\u001B[39m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;124m    Explicit color(s) for the arrows. If *C* has been set, *color* has no\u001B[39m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;124m    effect.\u001B[39m\n\u001B[0;32m    185\u001B[0m \n\u001B[0;32m    186\u001B[0m \u001B[38;5;124m    This is a synonym for the `.PolyCollection` *facecolor* parameter.\u001B[39m\n\u001B[0;32m    187\u001B[0m \n\u001B[0;32m    188\u001B[0m \u001B[38;5;124mOther Parameters\u001B[39m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;124m----------------\u001B[39m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;124mdata : indexable object, optional\u001B[39m\n\u001B[0;32m    191\u001B[0m \u001B[38;5;124m    DATA_PARAMETER_PLACEHOLDER\u001B[39m\n\u001B[0;32m    192\u001B[0m \n\u001B[0;32m    193\u001B[0m \u001B[38;5;124m**kwargs : `~matplotlib.collections.PolyCollection` properties, optional\u001B[39m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;124m    All other keyword arguments are passed on to `.PolyCollection`:\u001B[39m\n\u001B[0;32m    195\u001B[0m \n\u001B[0;32m    196\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m(PolyCollection:kwdoc)s\u001B[39m\n\u001B[0;32m    197\u001B[0m \n\u001B[0;32m    198\u001B[0m \u001B[38;5;124mReturns\u001B[39m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;124m-------\u001B[39m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;124m`~matplotlib.quiver.Quiver`\u001B[39m\n\u001B[0;32m    201\u001B[0m \n\u001B[0;32m    202\u001B[0m \u001B[38;5;124mSee Also\u001B[39m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;124m--------\u001B[39m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;124m.Axes.quiverkey : Add a key to a quiver plot.\u001B[39m\n\u001B[0;32m    205\u001B[0m \n\u001B[0;32m    206\u001B[0m \u001B[38;5;124mNotes\u001B[39m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;124m-----\u001B[39m\n\u001B[0;32m    208\u001B[0m \n\u001B[0;32m    209\u001B[0m \u001B[38;5;124m**Arrow shape**\u001B[39m\n\u001B[0;32m    210\u001B[0m \n\u001B[0;32m    211\u001B[0m \u001B[38;5;124mThe arrow is drawn as a polygon using the nodes as shown below. The values\u001B[39m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;124m*headwidth*, *headlength*, and *headaxislength* are in units of *width*.\u001B[39m\n\u001B[0;32m    213\u001B[0m \n\u001B[0;32m    214\u001B[0m \u001B[38;5;124m.. image:: /_static/quiver_sizes.svg\u001B[39m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;124m   :width: 500px\u001B[39m\n\u001B[0;32m    216\u001B[0m \n\u001B[0;32m    217\u001B[0m \u001B[38;5;124mThe defaults give a slightly swept-back arrow. Here are some guidelines how to\u001B[39m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;124mget other head shapes:\u001B[39m\n\u001B[0;32m    219\u001B[0m \n\u001B[0;32m    220\u001B[0m \u001B[38;5;124m- To make the head a triangle, make *headaxislength* the same as *headlength*.\u001B[39m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;124m- To make the arrow more pointed, reduce *headwidth* or increase *headlength*\u001B[39m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;124m  and *headaxislength*.\u001B[39m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;124m- To make the head smaller relative to the shaft, scale down all the head\u001B[39m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;124m  parameters proportionally.\u001B[39m\n\u001B[0;32m    225\u001B[0m \u001B[38;5;124m- To remove the head completely, set all *head* parameters to 0.\u001B[39m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;124m- To get a diamond-shaped head, make *headaxislength* larger than *headlength*.\u001B[39m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124m- Warning: For *headaxislength* < (*headlength* / *headwidth*), the \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheadaxis\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;124m  nodes (i.e. the ones connecting the head with the shaft) will protrude out\u001B[39m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;124m  of the head in forward direction so that the arrow head looks broken.\u001B[39m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m \u001B[38;5;241m%\u001B[39m _docstring\u001B[38;5;241m.\u001B[39minterpd\u001B[38;5;241m.\u001B[39mparams\n\u001B[0;32m    232\u001B[0m _docstring\u001B[38;5;241m.\u001B[39minterpd\u001B[38;5;241m.\u001B[39mupdate(quiver_doc\u001B[38;5;241m=\u001B[39m_quiver_doc)\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mQuiverKey\u001B[39;00m(martist\u001B[38;5;241m.\u001B[39mArtist):\n",
      "File \u001B[1;32mC:\\Data_Anaconda\\Lib\\site-packages\\matplotlib\\_docstring.py:60\u001B[0m, in \u001B[0;36m_ArtistKwdocLoader.__missing__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28mcls\u001B[39m, \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m _api\u001B[38;5;241m.\u001B[39mrecursive_subclasses(Artist)\n\u001B[0;32m     58\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m name]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m---> 60\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetdefault(key, kwdoc(\u001B[38;5;28mcls\u001B[39m))\n",
      "\u001B[1;31mKeyError\u001B[0m: 'PolyCollection:kwdoc'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#IG\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import log2\n",
    "\n",
    "# 加载数据：读取 .txt 文件并解析为 DataFrame 格式\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        # 使用制表符或空格分割文本和标签\n",
    "                        if '\\t' in line:\n",
    "                            text, label = line.rsplit('\\t', 1)\n",
    "                        else:\n",
    "                            text, label = line.rsplit(' ', 1)\n",
    "                        data.append({'text': text.strip(), 'label': int(label)})\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping invalid line: {line}\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# 清理文本：移除 \"#\" 和 \"@\" 后的内容，以及指定无意义词语和特定模式\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # 移除 \"#\" 和 \"@\" 后的内容\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # 移除以 \"#\" 开头的单词（如 \"#话题\"）\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # 移除以 \"@\" 开头的单词（如 \"@用户\"）\n",
    "    return text\n",
    "\n",
    "# 从 word.txt 中加载高频词\n",
    "def load_words(word_file):\n",
    "    words = []\n",
    "    with open(word_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = re.search(r'Word:\\s*(\\S+),', line)  # 提取 \"Word:\" 后面的部分\n",
    "            if match:\n",
    "                words.append(match.group(1))\n",
    "    return words\n",
    "\n",
    "# 计算熵\n",
    "def entropy(probabilities):\n",
    "    return -sum(p * log2(p) for p in probabilities if p > 0)\n",
    "\n",
    "# 计算信息增益\n",
    "def compute_information_gain(word, group_0_texts, group_1_texts, total_texts):\n",
    "    # 自杀组和非自杀组包含该词的文本数\n",
    "    N11 = sum(1 for text in group_0_texts if word in text)  # 自杀组包含该词的数量\n",
    "    N01 = sum(1 for text in group_1_texts if word in text)  # 非自杀组包含该词的数量\n",
    "\n",
    "    # 自杀组和非自杀组不包含该词的文本数\n",
    "    N10 = len(group_0_texts) - N11  # 自杀组不包含该词的数量\n",
    "    N00 = len(group_1_texts) - N01  # 非自杀组不包含该词的数量\n",
    "\n",
    "    # 总计数\n",
    "    total_group_0 = len(group_0_texts)\n",
    "    total_group_1 = len(group_1_texts)\n",
    "\n",
    "    # 类别分布概率（先验概率）\n",
    "    P_group_0 = total_group_0 / total_texts\n",
    "    P_group_1 = total_group_1 / total_texts\n",
    "\n",
    "    # 总熵 H(T)\n",
    "    H_T = entropy([P_group_0, P_group_1])\n",
    "\n",
    "    # 条件概率 P(w)\n",
    "    P_w = (N11 + N01) / total_texts\n",
    "    P_not_w = (N10 + N00) / total_texts\n",
    "\n",
    "    # 条件熵 H(T|w)\n",
    "    H_T_given_w = 0\n",
    "    H_T_given_not_w = 0\n",
    "\n",
    "    if P_w > 0:\n",
    "        P_T_given_w = [N11 / (N11 + N01), N01 / (N11 + N01)]\n",
    "        H_T_given_w += P_w * entropy(P_T_given_w)\n",
    "\n",
    "    if P_not_w > 0:\n",
    "        P_T_given_not_w = [N10 / (N10 + N00), N00 / (N10 + N00)]\n",
    "        H_T_given_not_w += P_not_w * entropy(P_T_given_not_w)\n",
    "\n",
    "    IG_value = H_T - (H_T_given_w + H_T_given_not_w)\n",
    "\n",
    "    return IG_value, N11, N01\n",
    "\n",
    "# 主函数：加载数据、计算信息增益并归一化结果\n",
    "def main():\n",
    "    # 文件路径\n",
    "    train_file_path = '/kaggle/input/shyan3-4/train.txt'\n",
    "    dev_file_path = '/kaggle/input/shyan3-4/dev.txt'\n",
    "    word_file_path = '/kaggle/input/high-freq-word/1-LLR-V.txt'\n",
    "\n",
    "    # 加载数据集并合并 train 和 dev 数据集\n",
    "    df = load_data([train_file_path, dev_file_path])\n",
    "    \n",
    "    # 清理文本内容\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "    # 分组文本\n",
    "    group_0_texts = df[df['label'] == 0]['text'].tolist()  # 自杀组文本列表\n",
    "    group_1_texts = df[df['label'] == 1]['text'].tolist()  # 非自杀组文本列表\n",
    "\n",
    "    total_texts = len(df)  # 总文本数量\n",
    "\n",
    "    # 加载高频词\n",
    "    words = load_words(word_file_path)\n",
    "\n",
    "    # 存储计算结果\n",
    "    results = []\n",
    "\n",
    "    print(\"\\nCalculating Information Gain for each word...\\n\")\n",
    "\n",
    "    for word in words:\n",
    "        ig_value, count_group_0, count_group_1 = compute_information_gain(\n",
    "            word, group_0_texts, group_1_texts, total_texts)\n",
    "\n",
    "        # 归一化为百分比形式\n",
    "        total_count = count_group_0 + count_group_1\n",
    "        if total_count > 0:\n",
    "            percentage_group_0 = (count_group_0 / total_count) * 100\n",
    "            percentage_group_1 = (count_group_1 / total_count) * 100\n",
    "        else:\n",
    "            percentage_group_0 = percentage_group_1 = 0\n",
    "\n",
    "        results.append({\n",
    "            'word': word,\n",
    "            'information_gain': ig_value,\n",
    "            'percentage_suicide': f\"{percentage_group_0:.2f}%\",\n",
    "            'percentage_non_suicide': f\"{percentage_group_1:.2f}%\"\n",
    "        })\n",
    "\n",
    "        print(f\"Word: {word}, IG: {ig_value:.4f}, Suicide: {percentage_group_0:.2f}%, Non-Suicide: {percentage_group_1:.2f}%\")\n",
    "\n",
    "    # 将结果保存为 CSV 文件或打印输出\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('word_information_gain4.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    print(\"\\nResults saved to 'word_information_gain3.csv'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "84a4428efb2fb35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import jieba.posseg as pseg  # 用于分词和 POS 标注\n",
    "import re  # 用于处理正则表达式\n",
    "\n",
    "# 自定义停用词列表（可以扩展）\n",
    "CUSTOM_STOPWORDS = {\"图片\", \"视频\", \"链接\", \"原图\", \"全文\", \"网页链接\"}\n",
    "\n",
    "# 加载数据：读取 .txt 文件并解析为 DataFrame 格式\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        # 使用制表符或空格分割文本和标签\n",
    "                        if '\\t' in line:\n",
    "                            text, label = line.rsplit('\\t', 1)\n",
    "                        else:\n",
    "                            text, label = line.rsplit(' ', 1)\n",
    "                        data.append({'text': text.strip(), 'label': int(label)})\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping invalid line: {line}\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# 清理文本：移除 \"#\" 和 \"@\" 后的内容，以及指定无意义词语和特定模式\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 移除 \"#\" 和 \"@\" 后的内容\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # 移除以 \"#\" 开头的单词（如 \"#话题\"）\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # 移除以 \"@\" 开头的单词（如 \"@用户\"）\n",
    "\n",
    "    # 移除类似 \"xxx的微博视频\" 的模式，其中 xxx 是任意字符\n",
    "    text = re.sub(r\"\\S+的微博视频\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 分组统计指定类别的频次，并获取高频单字\n",
    "def compute_pos_statistics(df, group_column, text_column):\n",
    "    pos_statistics = {}\n",
    "    \n",
    "    # 定义目标 POS 标签映射表\n",
    "    target_pos_tags = {\n",
    "        'Noun': ['n', 'nr', 'ns', 'nt', 'nz'],           # 名词及其子类，如人名、地名等\n",
    "        'Verb': ['v', 'vd', 'vn'],                       # 动词及其子类，如动名词等\n",
    "        'Pronouns': ['r'],                               # 代词，如“我”、“你”\n",
    "        'Adjectives': ['a', 'ad', 'an'],                 # 形容词及其子类，如副形容词等\n",
    "        'Adverbs': ['d'],                                # 副词，如“很”、“非常”\n",
    "        'Prepositions': ['p'],                           # 介词，如“在”、“从”\n",
    "        'Conjunctions': ['c']                            # 连词，如“和”、“但是”\n",
    "    }\n",
    "\n",
    "    for group in df[group_column].unique():\n",
    "        group_data = df[df[group_column] == group][text_column]\n",
    "\n",
    "        pos_counts = {pos: Counter() for pos in target_pos_tags}  # 初始化计数字典\n",
    "\n",
    "        for text in group_data:\n",
    "            if isinstance(text, str):  # 确保是字符串类型\n",
    "                cleaned_text = clean_text(text)  # 清理文本\n",
    "                \n",
    "                words_with_pos = pseg.cut(cleaned_text)  # 使用 jieba.posseg 对清理后的文本进行分词和 POS 标注\n",
    "                \n",
    "                for word, tag in words_with_pos:\n",
    "                    if word not in CUSTOM_STOPWORDS and word.strip():  # 排除停用词和空白字符\n",
    "                        for pos_category, pos_tags in target_pos_tags.items():\n",
    "                            if tag in pos_tags:  # 如果当前标注属于目标 POS 类别，则计数\n",
    "                                pos_counts[pos_category][word] += 1\n",
    "\n",
    "        total_count = sum(sum(counter.values()) for counter in pos_counts.values())\n",
    "\n",
    "        pos_statistics[group] = {\n",
    "            'pos_counts': pos_counts,\n",
    "            'total_count': total_count,\n",
    "            'percentage': {\n",
    "                pos_category: {word: (count / total_count * 100) for word, count in counter.items()}\n",
    "                for pos_category, counter in pos_counts.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return pos_statistics\n",
    "\n",
    "\n",
    "# 保存统计结果到 CSV 文件，每组每个类别生成一个文件\n",
    "def save_to_csv_by_group_and_category(pos_statistics, output_dir=\"output\"):\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for group, stats in pos_statistics.items():\n",
    "        group_name = \"Suicide_Risk\" if group == 0 else \"No_Suicide_Risk\"\n",
    "        \n",
    "        for pos_category, counter in stats['pos_counts'].items():\n",
    "            all_data = []\n",
    "            \n",
    "            for word, freq in counter.items():\n",
    "                percentage = stats['percentage'][pos_category][word]\n",
    "                all_data.append({\n",
    "                    \"Word\": word,\n",
    "                    \"Frequency\": freq,\n",
    "                    \"Percentage (%)\": f\"{percentage:.4f}\"\n",
    "                })\n",
    "            \n",
    "            df_output = pd.DataFrame(all_data)\n",
    "            \n",
    "            category_file_name = f\"{group_name}_{pos_category}_Statistics.csv\"\n",
    "            output_file_path = os.path.join(output_dir, category_file_name)\n",
    "            \n",
    "            df_output.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            print(f\"Saved results to {output_file_path}\")\n",
    "\n",
    "\n",
    "# 主函数：加载数据并计算结果，并生成 CSV 输出\n",
    "def main():\n",
    "    # 文件路径\n",
    "    file_paths = ['/kaggle/input/cipintongji/dev.txt', '/kaggle/input/cipintongji/train.txt']  # 替换为实际文件路径\n",
    "\n",
    "    # 加载数据\n",
    "    df = load_data(file_paths)\n",
    "\n",
    "    print(\"\\nLoaded Data:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # 计算词性统计信息\n",
    "    pos_statistics = compute_pos_statistics(df, group_column='label', text_column='text')\n",
    "\n",
    "    print(\"\\nSaving POS Statistics to CSV files...\")\n",
    "    \n",
    "    save_to_csv_by_group_and_category(pos_statistics)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "dfee60ffa42f281d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#人称代词 数据存入到 \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import jieba.posseg as pseg  # 用于分词和 POS 标注\n",
    "import re  # 用于处理正则表达式\n",
    "\n",
    "# 自定义停用词列表（可以扩展）\n",
    "CUSTOM_STOPWORDS = {\"图片\", \"视频\", \"链接\", \"原图\", \"全文\", \"网页链接\"}\n",
    "\n",
    "# 定义代词类别及对应的词汇表\n",
    "PRONOUN_CATEGORIES = {\n",
    "    'First Person Singular': {\"我\", \"自己\", \"俺\", \"咱\"},\n",
    "    'First Person Plural': {\"我们\", \"咱们\"},\n",
    "    'Second Person': {\"你\", \"你们\", \"您\", \"您们\"},\n",
    "    'Third Person': {\"他\", \"她\", \"他们\", \"她们\", \"爸爸\", \"妈妈\", \"哥哥\", \"姐姐\", \"叔叔\", \"大伯\"}\n",
    "}\n",
    "\n",
    "# 加载数据：读取 .txt 文件并解析为 DataFrame 格式\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        # 使用制表符或空格分割文本和标签\n",
    "                        if '\\t' in line:\n",
    "                            text, label = line.rsplit('\\t', 1)\n",
    "                        else:\n",
    "                            text, label = line.rsplit(' ', 1)\n",
    "                        data.append({'text': text.strip(), 'label': int(label)})\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping invalid line: {line}\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# 清理文本：移除 \"#\" 和 \"@\" 后的内容，以及指定无意义词语和特定模式\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 移除 \"#\" 和 \"@\" 后的内容\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # 移除以 \"#\" 开头的单词（如 \"#话题\"）\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # 移除以 \"@\" 开头的单词（如 \"@用户\"）\n",
    "\n",
    "    # 移除类似 \"xxx的微博视频\" 的模式，其中 xxx 是任意字符\n",
    "    text = re.sub(r\"\\S+的微博视频\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 分组统计代词类别的频次，并获取高频单字\n",
    "def compute_pronoun_statistics(df, group_column, text_column):\n",
    "    pronoun_statistics = {}\n",
    "\n",
    "    for group in df[group_column].unique():\n",
    "        group_data = df[df[group_column] == group][text_column]\n",
    "\n",
    "        pronoun_counts = {category: Counter() for category in PRONOUN_CATEGORIES}  # 初始化计数字典\n",
    "\n",
    "        for text in group_data:\n",
    "            if isinstance(text, str):  # 确保是字符串类型\n",
    "                cleaned_text = clean_text(text)  # 清理文本\n",
    "                \n",
    "                words_with_pos = pseg.cut(cleaned_text)  # 使用 jieba.posseg 对清理后的文本进行分词和 POS 标注\n",
    "                \n",
    "                for word, tag in words_with_pos:\n",
    "                    if word not in CUSTOM_STOPWORDS and word.strip() and tag == 'r':  # 排除非代词和空白字符\n",
    "                        for category, pronouns in PRONOUN_CATEGORIES.items():\n",
    "                            if word in pronouns:  # 如果当前单词属于某个代词类别，则计数\n",
    "                                pronoun_counts[category][word] += 1\n",
    "\n",
    "        total_count = sum(sum(counter.values()) for counter in pronoun_counts.values())\n",
    "\n",
    "        pronoun_statistics[group] = {\n",
    "            'pronoun_counts': pronoun_counts,\n",
    "            'total_count': total_count,\n",
    "            'percentage': {\n",
    "                category: {word: (count / total_count * 100) for word, count in counter.items()}\n",
    "                for category, counter in pronoun_counts.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return pronoun_statistics\n",
    "\n",
    "\n",
    "# 保存统计结果到 CSV 文件，每组每类生成一个文件\n",
    "def save_to_csv_by_group_and_category(pronoun_statistics, output_dir=\"output1\"):\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for group, stats in pronoun_statistics.items():\n",
    "        group_name = \"Suicide_Risk\" if group == 0 else \"No_Suicide_Risk\"\n",
    "        \n",
    "        for category, counter in stats['pronoun_counts'].items():\n",
    "            all_data = []\n",
    "            \n",
    "            for word, freq in counter.items():\n",
    "                percentage = stats['percentage'][category][word]\n",
    "                all_data.append({\n",
    "                    \"Word\": word,\n",
    "                    \"Frequency\": freq,\n",
    "                    \"Percentage (%)\": f\"{percentage:.4f}\"\n",
    "                })\n",
    "            \n",
    "            df_output = pd.DataFrame(all_data)\n",
    "            \n",
    "            category_file_name = f\"{group_name}_{category.replace(' ', '_')}_Statistics.csv\"\n",
    "            output_file_path = os.path.join(output_dir, category_file_name)\n",
    "            \n",
    "            df_output.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            print(f\"Saved results to {output_file_path}\")\n",
    "\n",
    "\n",
    "# 主函数：加载数据并计算结果，并生成 CSV 输出\n",
    "def main():\n",
    "    file_paths = ['/kaggle/input/cipintongji/dev.txt', '/kaggle/input/cipintongji/train.txt']  # 替换为实际文件路径\n",
    "\n",
    "    # 加载数据（假设文件格式为“一段中文 后面标有0或1”）\n",
    "    df = load_data(file_paths)\n",
    "\n",
    "    print(\"\\nLoaded Data:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # 分析每组中各类代词的统计信息，仅针对目标类别进行分析，并清理无意义内容\n",
    "    pronoun_statistics = compute_pronoun_statistics(df, group_column='label', text_column='text')\n",
    "\n",
    "    print(\"\\nSaving Pronoun Statistics to CSV files...\")\n",
    "    \n",
    "    save_to_csv_by_group_and_category(pronoun_statistics)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "3aa092605b8a0fe1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
